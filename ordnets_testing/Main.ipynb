{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8352ec1-2f94-46fc-a165-0f37273c85fb",
   "metadata": {},
   "source": [
    "## TODO\n",
    " - Anáise de dados\n",
    " - Continuar testes (Dot Product Loss e Neg Sampling, Modelo de Classificação padrão)\n",
    " \n",
    "## DONE\n",
    " - Fixar permutação (https://gitlab.com/cristophersfr/fisher-networks/-/blob/corrected/Corrections_on_Scientific_Reports.pdf) \n",
    " - n = 5 \n",
    " - features como binário da permutação \n",
    " - testar outra métrica (pytorch metric learning) (As listas de adjacência possuem tamanhos diferentes entre os grafos, então não conseguimos concatenar os tensores para fazer treinamento em batch, dai usar o pytorch metric learning por agora não é possível (podemos fazer um padding nas listas, mas isso aumentaria significativamente o gasto com espaço). Vou continuar a investigar outras alternativas para conseguir usar a biblioteca)\n",
    " \n",
    "## Situação atual\n",
    " - O modelo continua sem conseguir otimizar. Atualmente estamos utilizando as features binárias e testando variações do GAT e do GCN, mas o modelo não consegue otimizar (embeddings vão todos para vetor de zeros). Ainda existe muita experimentação possível com as arquiteturas, então acredito que com mais testes essa situação mude.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a1844e-2fc2-429e-be3d-54806cc2fce5",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d13773-6c71-4cae-9e16-9d3f51293745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import torch as th\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn import GCNConv,SAGEConv,aggr,GATConv\n",
    "from torch_geometric.nn.pool import global_max_pool\n",
    "import ordpy\n",
    "import numpy as np\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy import sparse\n",
    "from Data import *\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import itertools\n",
    "\n",
    "from pytorch_metric_learning import distances, losses, miners, reducers, testers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3f87ec-4c6a-4231-9bd5-f2910ce6fe3f",
   "metadata": {},
   "source": [
    "## Creating Data\n",
    "Creates a data.pkl file with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2a167f-c434-4f7c-bf0f-9eb4e80ed97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8571325-bf6d-4d1c-a365-7fe8dcc1064f",
   "metadata": {},
   "source": [
    "## Pre-Processing: Creating Ordinal Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56205250-053a-4d1c-a81b-ddde4384b2bf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1da863-21fd-47fb-b99d-60246c160ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ord_net(time_series:list,ord_net_dim:int,label_encoder=None):\n",
    "    # Creates the ordinal network for a given time_series\n",
    "    # Returns nodes as a list of strings (e.g. ['12345','12354',...])\n",
    "    # Returns label-encoded edges and float weights\n",
    "    nodes,edges,weights = ordpy.ordinal_network(time_series,ord_net_dim)\n",
    "    \n",
    "    if label_encoder != None:\n",
    "        label_encoder.fit(nodes)\n",
    "        edges = th.Tensor(np.array([label_encoder.transform(edges[:,0]),label_encoder.transform(edges[:,1])])).long()\n",
    "        nodes = [node.replace(\"|\",\"\") for node in nodes]\n",
    "        \n",
    "    return nodes,edges,weights\n",
    "    \n",
    "def create_graph_list(series_list:list,ord_net_dim:int):\n",
    "    # Creates the list of ordinal networks from a list of time series\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    graphs = []\n",
    "    for series in tqdm(series_list):\n",
    "        nodes,edges,weights = create_ord_net(series,ord_net_dim,le)\n",
    "        graphs.append([th.Tensor(edges),th.Tensor(weights)])\n",
    "    pkl.dump(graphs,open(save_path,\"wb\"))\n",
    "    \n",
    "    return graphs\n",
    "\n",
    "def create_bin_features(ord_net_dim:int,num_bits:int):\n",
    "    # Create a binary representation of the permutations as a feature matrix\n",
    "    permutation_list = list(itertools.permutations([str(i) for i in range(1,ord_net_dim+1)]))\n",
    "    permutation_list = [[np.binary_repr(int(i),width=num_bits) for i in curr_list] for curr_list in permutation_list]\n",
    "    node_feats = np.stack([np.array(list(''.join(bit_list)),dtype=int) for bit_list in permutation_list],axis=0)\n",
    "    return th.Tensor(node_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe5c81e-3dd1-4b00-b56a-36472398830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(th.utils.data.Dataset):\n",
    "    # Pytorch Wrapper\n",
    "    def __init__(self,graphs,labels):\n",
    "        self.graphs = graphs\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.graphs[idx],self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aba3fa-660b-4054-ae02-fa55fde763b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EdgeList2AdjMatrix(edges,weights):\n",
    "    adj_matrix = np.zeros([num_nodes,num_nodes],dtype=float)\n",
    "\n",
    "    for edge_idx in range(edges.shape[1]):\n",
    "        adj_matrix[edges[0,edge_idx].item(),edges[1,edge_idx].item()] = weights[edge_idx]\n",
    "    return adj_matrix\n",
    "\n",
    "\n",
    "def optimal_ordering(edges,weights,node_feats,return_adj:bool=False):\n",
    "    adj_matrix = EdgeList2AdjMatrix(edges,weights)\n",
    "    \n",
    "    Z = hierarchy.ward(adj_matrix)\n",
    "    opt_ordering = hierarchy.leaves_list(hierarchy.optimal_leaf_ordering(Z,adj_matrix))\n",
    "    \n",
    "    ordered_feats = node_feats[opt_ordering,:]\n",
    "    if return_adj:\n",
    "        identifier_arrays = []\n",
    "        for idx in opt_ordering:\n",
    "            curr_array = np.zeros(num_nodes)\n",
    "            curr_array[idx] = 1\n",
    "            identifier_arrays.append(curr_array)\n",
    "\n",
    "        permutation_matrix = np.stack(arrays)\n",
    "        ordered_adj = permutation_matrix @ adj_matrix @ permutation_matrix.T\n",
    "    \n",
    "        return ordered_adj,ordered_feats\n",
    "    else:\n",
    "        permutation_dict = {opt_ordering[i]:i for i in range(len(opt_ordering))}\n",
    "        src_edges = th.Tensor([permutation_dict[edge.item()] for edge in edges[0,:]]).int()\n",
    "        tgt_edges = th.Tensor([permutation_dict[edge.item()] for edge in edges[1,:]]).int()\n",
    "\n",
    "        ordered_edges = th.stack([src_edges,tgt_edges])\n",
    "        ordered_weights = weights[opt_ordering]\n",
    "        \n",
    "        return ordered_edges,ordered_weights,ordered_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d500648-3caa-4687-ae21-cd707cdfac51",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9635ef12-e298-4bc6-9f40-0ac4844b681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Parameters\n",
    "LOAD_GRAPHS = True\n",
    "USE_BINARY_FEATURES = True # If True, will use a binary representation of a given permutation as the node feature\n",
    "ORDER_EDGES = False # If True, will order edges according to the optimal leaf ordering\n",
    "\n",
    "# Pre-Processing Parameters\n",
    "ord_net_dim = 5 # Controls the sliding window size in the creation of the ordinal network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2591b970-625f-4b8d-a029-c68fcccdc5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing graph parameters and creating node features\n",
    "\n",
    "num_nodes = np.math.factorial(ord_net_dim)\n",
    "num_bits = int(np.ceil(ord_net_dim / 2))\n",
    "feat_dim = num_bits * ord_net_dim\n",
    "save_path = \"graphs\"\n",
    "\n",
    "if USE_BINARY_FEATURES:\n",
    "    save_path += \"_binary_feats\"\n",
    "    node_feats = create_bin_features(ord_net_dim,num_bits)\n",
    "else:\n",
    "    node_feats = th.ones([num_nodes,feat_dim])\n",
    "        \n",
    "save_path += \".pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf37107-f041-4199-b47e-45cd9b9c01ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_data,labels,true_pos = pkl.load(open(\"data.pkl\", \"rb\"))\n",
    "\n",
    "if LOAD_GRAPHS:\n",
    "    graphs = pkl.load(open(save_path, \"rb\"))\n",
    "else:\n",
    "    graphs = create_graph_list(series_data,ord_net_dim) \n",
    "\n",
    "if ORDER_EDGES:\n",
    "    for idx,(edges,weights) in tqdm(enumerate(graphs)):\n",
    "        ordered_edges,ordered_weights,ordered_feats = optimal_ordering(edges,weights,node_feats)\n",
    "        graphs[idx] = [ordered_edges,ordered_weights,ordered_feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a9838-f98e-4c0a-8a4c-7ba9fd8bda54",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8339e2ab-c2cc-402a-838a-8ec38fca05cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eadcaf1-12ac-4d8c-bb0f-558567ce74e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels= np.array(labels)\n",
    "\n",
    "idx = 1\n",
    "label_list = [0,30]\n",
    "\n",
    "for i in tqdm(label_list):\n",
    "    idxs = np.where(labels == i)[0]\n",
    "    tgt_weights = [graphs[idx][1] for idx in idxs]\n",
    "\n",
    "    for weights in tgt_weights:\n",
    "        plt.hist(weights.numpy(),bins=20,label=f\"graph({i})_{idx}\")\n",
    "        idx += 1\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3254a14a-3d8f-4dc3-a480-5d122d194ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af47dd9f-7309-4241-bf5e-2a6d21e4003b",
   "metadata": {},
   "source": [
    "## Training GNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfe9719-1c99-4881-8dd2-f35af029666b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ce04c-7a69-44a6-a624-768e7dec8afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(th.nn.Module):\n",
    "    def __init__(self,layers:list,out_dim:int=None,skip_connect:bool=False,**layer_kwargs):\n",
    "        super().__init__()\n",
    "        self.num_layers = len(layers)\n",
    "        self.convs = th.nn.ModuleList()\n",
    "        for layer_idx in range(self.num_layers-1):\n",
    "            self.convs.append(GCNConv(layers[layer_idx],layers[layer_idx+1],**layer_kwargs))\n",
    "        self.out_dim = out_dim if out_dim != None else layers[-1]\n",
    "        self.linear = th.nn.Linear(layers[-1],self.out_dim)\n",
    "        self.skip_connect = skip_connect\n",
    "            \n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor,edge_weights: Tensor,agg_func,**agg_kwargs) -> Tensor:\n",
    "        # x: Node feature matrix of shape [num_nodes, in_channels]\n",
    "        # edge_index: Graph connectivity matrix of shape [2, num_edges]\n",
    "        embeddings = []\n",
    "        for conv_layer in self.convs:\n",
    "            x = conv_layer(x, edge_index,edge_weights).relu()\n",
    "            embeddings.append(x)\n",
    "        \n",
    "        if self.skip_connect:\n",
    "            x = th.stack([agg_func(embedding,**agg_kwargs).flatten() for embedding in embeddings])\n",
    "\n",
    "        return self.linear(agg_func(x,**agg_kwargs).flatten())\n",
    "\n",
    "class GAT(th.nn.Module):\n",
    "    def __init__(self,layers:list,out_dim:int=None,skip_connect:bool=False,**layer_kwargs):\n",
    "        super().__init__()\n",
    "        self.num_layers = len(layers)\n",
    "        self.convs = th.nn.ModuleList()\n",
    "        for layer_idx in range(self.num_layers-1):\n",
    "            self.convs.append(GATConv(layers[layer_idx],layers[layer_idx+1],**layer_kwargs))\n",
    "        self.out_dim = out_dim if out_dim != None else layers[-1]\n",
    "        self.linear = th.nn.Linear(layers[-1],self.out_dim)\n",
    "        self.skip_connect = skip_connect\n",
    "            \n",
    "\n",
    "    def forward(self, x: Tensor, edge_index: Tensor,edge_weights: Tensor,agg_func,**agg_kwargs) -> Tensor:\n",
    "        # x: Node feature matrix of shape [num_nodes, in_channels]\n",
    "        # edge_index: Graph connectivity matrix of shape [2, num_edges]\n",
    "        embeddings = []\n",
    "        for conv_layer in self.convs:\n",
    "            x = conv_layer(x, edge_index,edge_attr = edge_weights).relu()\n",
    "            embeddings.append(x)\n",
    "        \n",
    "        if self.skip_connect:\n",
    "            x = th.stack([agg_func(embedding,**agg_kwargs).flatten() for embedding in embeddings])\n",
    "\n",
    "        return self.linear(agg_func(x,**agg_kwargs).flatten())    \n",
    "    \n",
    "        \n",
    "class tripletLoss(th.nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(tripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, pos, neg):\n",
    "        distance_pos = (anchor - pos).pow(2).sum()\n",
    "        distance_neg = (anchor - neg).pow(2).sum()\n",
    "        loss = th.nn.functional.relu(distance_pos - distance_neg + self.margin)\n",
    "        return loss.mean(), self.triplet_correct(distance_pos, distance_neg)\n",
    "\n",
    "    def triplet_correct(self, d_pos, d_neg):\n",
    "        return (d_pos < d_neg).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ff772-decc-4049-9cd5-7786ab79d7cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f177ec97-bce5-4d19-ad0b-5e5cb835e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "USE_GPU = False\n",
    "num_epochs = 100\n",
    "batch_size = 1\n",
    "gnn_layers_dim = [feat_dim,32,32]\n",
    "skip_connect = False # If True will add skip-connections to GCN (make sure all layer dimensions are the same!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9f07d1-97eb-404e-b418-5ccdbcd485da",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = th.device(\"cuda\") if (USE_GPU and th.cuda.is_available()) else th.device(\"cpu\")\n",
    "\n",
    "gnn_aggr_func = aggr.SoftmaxAggregation(learn=True)\n",
    "\n",
    "# model = GAT(gnn_layers_dim,heads=2,edge_dim=1).to(device)\n",
    "model = GCN(gnn_layers_dim,project=True).to(device)\n",
    "print(model)\n",
    "\n",
    "criterion = tripletLoss(margin = 1).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0001, momentum = 0.9, weight_decay = 0.2)\n",
    "test_indexes = cross_validation_sample(50, 10)\n",
    "\n",
    "node_feats = node_feats.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cc35e3-d6a5-4249-a3e0-e861a7b5c15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, test_index in enumerate(test_indexes):\n",
    "    # split training & testing\n",
    "    print(\"Test indexes: \", test_index)\n",
    "    train_x, train_y, test_x, test_y = split_colocation_train(graphs, labels, test_index, 'room')\n",
    "    train_x = gen_colocation_triplet(train_x, train_y)\n",
    "    test_x = gen_colocation_triplet(test_x, test_y)\n",
    "\n",
    "    total_triplets = len(train_x)\n",
    "    logging_step = total_triplets//10\n",
    "    print(\"Total training triplets: %d\\n\" % (total_triplets))\\\n",
    "    \n",
    "    # Training Loop\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        total_triplet_correct = 0\n",
    "        np.random.shuffle(train_x)\n",
    "        model.train()\n",
    "        for step, batch_x in enumerate(train_x):\n",
    "            anchor_edges,anchor_weights = batch_x[0]\n",
    "            anchor_pred = model(node_feats,anchor_edges.to(device),anchor_weights.to(device),gnn_aggr_func,dim=0)\n",
    "            \n",
    "            pos_edges,pos_weights = batch_x[1]\n",
    "            pos_pred = model(node_feats,pos_edges.to(device),pos_weights.to(device),gnn_aggr_func,dim=0)\n",
    "            \n",
    "            \n",
    "            neg_edges,neg_weights = batch_x[2]\n",
    "            neg_pred = model(node_feats,neg_edges.to(device),neg_weights.to(device),gnn_aggr_func,dim=0)\n",
    "            \n",
    "            loss, triplet_correct = criterion(anchor_pred, pos_pred, neg_pred)\n",
    "            total_triplet_correct += triplet_correct.item()\n",
    "            \n",
    "            optimizer.zero_grad()           \n",
    "            loss.backward()                 \n",
    "            optimizer.step()\n",
    "            \n",
    "            if step % logging_step == 0 and step != 0:\n",
    "                print(\"loss \"+str(loss.item())+\"\\n\")\n",
    "                print(\"triplet_acc \" + str(triplet_correct.item()/batch_size) + \"\\n\")\n",
    "            \n",
    "        print(f\"Epoch={epoch+1}\")\n",
    "        print(\"Triplet accuracy: %f\"%(total_triplet_correct/total_triplets))    \n",
    "\n",
    "    # Testing Loop\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        total_triplet_correct = 0\n",
    "        for step, batch_x in tqdm(enumerate(test_x)):\n",
    "            anchor_edges,anchor_weights = batch_x[0]\n",
    "            anchor_pred = model(node_feats,anchor_edges.to(device),anchor_weights.to(device),gnn_aggr_func,dim=0)\n",
    "            \n",
    "            pos_edges,pos_weights = batch_x[1]\n",
    "            pos_pred = model(node_feats,pos_edges.to(device),pos_weights.to(device),gnn_aggr_func,dim=0)\n",
    "            \n",
    "            \n",
    "            neg_edges,neg_weights = batch_x[2]\n",
    "            neg_pred = model(node_feats,neg_edges.to(device),neg_weights.to(device),gnn_aggr_func,dim=0)\n",
    "            \n",
    "            loss, triplet_correct = criterion(anchor_pred, pos_pred, neg_pred)\n",
    "            total_triplet_correct += triplet_correct.item()\n",
    "            \n",
    "        print(\"Test Triplet accuracy: %f\"%(total_triplet_correct/total_triplets))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda65f81-be9a-4364-b911-b6b8c94e8e68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
